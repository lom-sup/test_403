import requests
from datetime import datetime, timedelta
import logging
import os
import time
import pandas as pd

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.models.variable import Variable

class EverydayNYTDNews():
    
    def __init__(self):
        # NYT API ì„¤ì •
        self.API_KEY = Variable.get("NYTD_API_KEY") # âŒAirflow ë“±ë¡ë¡
        self.BASE_URL = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
        self.news_desks = ["Business", "Financial", "Market Place", "Business Day", "DealBook", "Personal Investing"]
        self.companies = ["Apple", "Amazon", "Google", "Microsoft", "Facebook", "Tesla", "Netflix"]
        self.wait_between_requests = 15
        self.fl_fields = "headline,pub_date,lead_paragraph,web_url,news_desk"
        self.news_desk_fq = 'news_desk:("' + '", "'.join(self.news_desks) + '")'
        
        
        # AWS S3 ì„¤ì •
        self.S3_bucket = "de5-finalproj-team5-test" # âŒìˆ˜ì •í•„ìš”
        self.S3_folder = "raw_data/NYTD"
        self.AWS_CONN_ID = "myaws"
        
    def fetch_page_data(self, url, params, max_retries=5):
        """API ìš”ì²­ ë° 429 ì˜¤ë¥˜ ëŒ€ë¹„ ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©"""
        retries = 0
        while retries < max_retries:
            try:
                response = requests.get(url, params=params)
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    wait_time = 60 * (2 ** retries)
                    print(f"âš ï¸ Rate limit hit (429). Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                    retries += 1
                else:
                    print(f"âŒ HTTP error {response.status_code} for URL: {response.url}")
                    return None
            except requests.exceptions.RequestException as e:
                print(f"âŒ Request error: {e}")
                return None
        return None
        
    def get_data(self, **context):
        all_articles = []
        execution_date = context["execution_date"]
        
        # ë‚ ì§œ(YYYYMMDD í˜•ì‹)
        # ì¼ì¼ ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ë³€ê²½
        year = execution_date.year
        month = str(execution_date.month).zfill(2)
        begin_date = f"{year}{month}{str(execution_date.day).zfill(2)}"
        end_date = f"{year}{month}{str(execution_date.day).zfill(2)}"
        
        for company in self.companies:
            print(f"ğŸ“¡ Fetching articles for {company}...")
            
            page = 0
            while True:
                params = {
                    "q": company,
                    "fq": self.news_desk_fq,
                    "begin_date": begin_date,
                    "end_date": end_date,
                    "sort": "oldest",
                    "page": page,
                    "fl": self.fl_fields,
                    "facet": "true",
                    "facet_fields": "news_desk",
                    "facet_filter": "true",
                    "api-key": self.API_KEY
                }

                data = self.fetch_page_data(self.BASE_URL, params)
                if not data:
                    break

                docs = data.get("response", {}).get("docs", [])
                if not docs:
                    break
                
                logging.info(f"{company} ë°ì´í„° ìˆ˜ì§‘ ì¤‘, í˜ì´ì§€ {page}")
                
                for doc in docs:
                    article = {
                        "headline": doc.get("headline", {}).get("main", ""),
                        "pub_date": doc.get("pub_date", ""),
                        "content": doc.get("lead_paragraph") or doc.get("snippet") or "",
                        "web_url": doc.get("web_url", ""),
                        "news_desk": doc.get("news_desk", "N/A"),
                        "stock": company
                    }
                    all_articles.append(article)
                    
                print(f"Fetched page {page} for {company}, {len(docs)} articles.")
                page += 1
                time.sleep(self.wait_between_requests)
                
            logging.info(f"{company} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")

        if all_articles:
            
            # Parquet íŒŒì¼ ì €ì¥
            df = pd.DataFrame(all_articles)
            file_path = f"/opt/airflow/data/nyt_articles_{year}_{month}_{begin_date}.parquet"
            
            # íŒŒì¼ ì €ì¥ ì „ì— ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì—†ìœ¼ë©´ ìƒì„±
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            # íŒŒì¼ ì €ì¥
            df.to_parquet(file_path, engine="pyarrow", index=False)
            
            # s3_key ë„˜ê²¨ì£¼ê¸°
            # s3_key = f"{self.S3_folder}/{year}/nyt_articles_{year}_{month}_{begin_date}.parquet"
            s3_key = f"raw_data/NYTD/{year}/nyt_articles_{year}_{month}_{begin_date}.parquet"
            
            # XComì— S3 Key ì €ì¥
            context["task_instance"].xcom_push(key="s3_key", value=s3_key)
            context["task_instance"].xcom_push(key="file_path", value= file_path)
        else:
            print("âš ï¸ No articles found for this date.")

    def s3_write(self, **context):
        s3_hook = S3Hook(aws_conn_id=self.AWS_CONN_ID)
        s3_key = context["task_instance"].xcom_pull(task_ids="task_get_nytd_data", key="s3_key")
        file_path = context["task_instance"].xcom_pull(task_ids="task_get_nytd_data", key="file_path")

        if not s3_key:
            print("âš ï¸  XComì—ì„œ s3_keyë¥¼ ë°›ì§€ ëª»í•¨, ì—…ë¡œë“œí•˜ì§€ ì•ŠìŒ.")
            return

        # S3ì— íŒŒì¼ ì—…ë¡œë“œ
        s3_hook.load_file(
            filename=file_path,
            key=s3_key,
            bucket_name=self.S3_bucket,
            replace=True
            )
        logging.info("S3 ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ")


# í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
nyt_news = EverydayNYTDNews()

# DAG ìƒì„±
default_args = {
    'owner': 'airflow',
    'depends_on_past': False, # Trueë¡œ?
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'everyday_nytd_s3',
    default_args=default_args,
    start_date=datetime(2025, 3, 1),
    schedule_interval="0 8 * * *",  #ë§¤ì¼ í•œ ë²ˆì”© ì˜¤ì „ 8ì‹œì— ì‹¤í–‰(ë¯¸ì¥ 9ì‹œ 30ë¶„ ê°œì¥)
    catchup=False
) as dag:
    
    task_get_nytd_data = PythonOperator(
        task_id='task_get_nytd_data',
        python_callable=nyt_news.get_data
    )
    
    task_s3_data_write = PythonOperator(
        task_id="task_s3_data_write",
        python_callable=nyt_news.s3_write,
    )
    
    task_get_nytd_data >> task_s3_data_write